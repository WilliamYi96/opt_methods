{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us test the efficiency of numpy and scipy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "\n",
    "from jupyter_utils import change_path_to_parent\n",
    "change_path_to_parent()\n",
    "\n",
    "from datasets import get_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sparse random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "dim = 2000\n",
    "A = csr_matrix(scipy.sparse.random(n, dim))\n",
    "x = csr_matrix(scipy.sparse.random(dim, 1))\n",
    "A_ = csc_matrix(A)\n",
    "x_ = csc_matrix(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare csc_matrix and csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 µs ± 13.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "369 µs ± 6.28 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "151 µs ± 2.42 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "99 µs ± 2.24 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit A.dot(x)\n",
    "%timeit A.dot(x_)\n",
    "%timeit A_.dot(x)\n",
    "%timeit A_.dot(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check that the results match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(A.dot(x) - A.dot(x_)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is Numpy faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.toarray().squeeze()\n",
    "A = A.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.23 ms ± 60.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit A @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us sample rows (used in SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_stochastic_gradient(A, x):\n",
    "    n = A.shape[0]\n",
    "    i = np.random.choice(n)\n",
    "    return A[i].dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.74 µs ± 56.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "72.2 ms ± 3.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "229 µs ± 8.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "322 µs ± 9.77 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fake_stochastic_gradient(A, x)\n",
    "%timeit fake_stochastic_gradient(A, x_)\n",
    "%timeit fake_stochastic_gradient(A_, x)\n",
    "%timeit fake_stochastic_gradient(A_, x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "### 1. Use csc for deterministic (full batch) gradient computation\n",
    "### 2. Use csr if stochastic gradients are required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiencty of other sparse-vector operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = csr_matrix([1] + list(np.zeros(1000000)))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 µs ± 2.56 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "39.2 µs ± 2.46 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "35.9 µs ± 1.61 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "38.1 µs ± 1.24 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "4.41 µs ± 140 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit x + x\n",
    "%timeit x * 2\n",
    "%timeit abs(x)\n",
    "%timeit x.minimum(0.5)\n",
    "%timeit x.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 999999 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = csr_matrix(np.arange(1000000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27 ms ± 130 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.9 ms ± 111 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.81 ms ± 36.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "2.52 ms ± 197 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "680 µs ± 39.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit x + x\n",
    "%timeit x * 2\n",
    "%timeit abs(x)\n",
    "%timeit x.minimum(0.5)\n",
    "%timeit x.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = get_dataset('news20_class1')\n",
    "# A = A.toarray()\n",
    "l1 = 1e-4\n",
    "loss = LogisticRegression(A, b, l1=l1, l2=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient1(loss, x, idx=None, batch_size=1, replace=False, normalization=None):\n",
    "    if idx is None:\n",
    "        idx = np.random.choice(loss.n, size=batch_size, replace=replace)\n",
    "    else:\n",
    "        batch_size = 1 if np.isscalar(idx) else len(idx)\n",
    "    if normalization is None:\n",
    "        normalization = batch_size\n",
    "    z = loss.A[idx] @ x\n",
    "    if scipy.sparse.issparse(z):\n",
    "        z = z.toarray().ravel()\n",
    "    activation = scipy.special.expit(z)\n",
    "    error = (activation-loss.b[idx]) / normalization\n",
    "    stoch_grad = safe_sparse_add(loss.A[idx].T@error, loss.l2*x)\n",
    "    return scipy.sparse.csr_matrix(stoch_grad).T\n",
    "\n",
    "def stochastic_gradient2(loss, x, idx=None, batch_size=1, replace=False, normalization=None):\n",
    "    if idx is None:\n",
    "        idx = np.random.choice(loss.n, size=batch_size, replace=replace)\n",
    "    else:\n",
    "        batch_size = 1 if np.isscalar(idx) else len(idx)\n",
    "    A_idx = loss.A[idx]\n",
    "    if normalization is None:\n",
    "        normalization = batch_size\n",
    "    z = A_idx @ x\n",
    "    if scipy.sparse.issparse(z):\n",
    "        z = z.toarray().ravel()\n",
    "    activation = scipy.special.expit(z)\n",
    "    if scipy.sparse.issparse(x):\n",
    "        error = csr_matrix(activation-loss.b[idx]) / normalization\n",
    "    else:\n",
    "        error = (activation-loss.b[idx]) / normalization\n",
    "    return loss.l2*x + (error@A_idx).T\n",
    "\n",
    "def stochastic_gradient3(loss, x, idx=None, batch_size=1, replace=False, normalization=None):\n",
    "    if idx is None:\n",
    "        idx = np.random.choice(loss.n, size=batch_size, replace=replace)\n",
    "    else:\n",
    "        batch_size = 1 if np.isscalar(idx) else len(idx)\n",
    "    if normalization is None:\n",
    "        normalization = batch_size\n",
    "    z = loss.A[idx] @ x\n",
    "    if scipy.sparse.issparse(z):\n",
    "        z = z.toarray().ravel()\n",
    "    activation = scipy.special.expit(z)\n",
    "    error = csc_matrix(activation-loss.b[idx]) / normalization\n",
    "    stoch_grad = safe_sparse_add(loss.l2*x.T, error@loss.A[idx])\n",
    "    return scipy.sparse.csr_matrix(stoch_grad).T\n",
    "\n",
    "def stochastic_gradient5(loss, x, idx=None, batch_size=1, replace=False, normalization=None):\n",
    "    if idx is None:\n",
    "        idx = np.random.choice(loss.n, size=batch_size, replace=replace)\n",
    "    else:\n",
    "        batch_size = 1 if np.isscalar(idx) else len(idx)\n",
    "    if normalization is None:\n",
    "        normalization = batch_size\n",
    "    z = loss.A[idx] @ x\n",
    "    if scipy.sparse.issparse(z):\n",
    "        z = z.toarray().ravel()\n",
    "    activation = scipy.special.expit(z)\n",
    "    error = csc_matrix(activation-loss.b[idx]) / normalization\n",
    "    stoch_grad = safe_sparse_add(loss.l2*x, (error@loss.A[idx]).T)\n",
    "    return scipy.sparse.csr_matrix(stoch_grad)\n",
    "\n",
    "def grad_step(loss, x, lr, batch_size=32, option=1):\n",
    "    if option == 1:\n",
    "        grad = stochastic_gradient1(loss, x, batch_size=batch_size)\n",
    "    elif option == 2:\n",
    "        grad = stochastic_gradient2(loss, x, batch_size=batch_size)\n",
    "    elif option == 3:\n",
    "        grad = stochastic_gradient3(loss, x, batch_size=batch_size)\n",
    "    elif option == 4:\n",
    "        grad = stochastic_gradient4(loss, x, batch_size=batch_size)\n",
    "    elif option == 5:\n",
    "        grad = stochastic_gradient5(loss, x, batch_size=batch_size)\n",
    "    elif option == 6:\n",
    "        grad = stochastic_gradient6(loss, x, batch_size=batch_size)\n",
    "    elif option == 7:\n",
    "        grad = stochastic_gradient7(loss, x, batch_size=batch_size)\n",
    "    elif option == 8:\n",
    "        grad = stochastic_gradient8(loss, x, batch_size=batch_size)\n",
    "    elif option == 9:\n",
    "        grad = stochastic_gradient9(loss, x, batch_size=batch_size)\n",
    "    return x - lr * grad\n",
    "\n",
    "def prox_grad_step(loss, x, lr, batch_size=32, option=1):\n",
    "    return loss.regularizer.prox(grad_step(loss, x, lr, batch_size, option), self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1 / loss.smoothness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.03 ms ± 35.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.59 ms ± 29.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.95 ms ± 92.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.56 ms ± 29.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "2.17 ms ± 47.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit grad_step(loss, gd.x, lr_, option=1)\n",
    "%timeit grad_step(loss, gd.x, lr_, option=2)\n",
    "%timeit grad_step(loss, gd.x, lr_, option=3)\n",
    "%timeit grad_step(loss, gd.x, lr_, option=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random number generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
